{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyansh-2603/Responsive-React-Navbar/blob/main/Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJv_5mLKEC_C",
        "outputId": "49d6b5d5-6860-4777-a516-9d21cdf34bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bmOHayqD89Kx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from imageio import  mimsave\n",
        "from IPython.display import display as display_fn\n",
        "from IPython.display import Image, clear_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "**Conversion of Tensor to image**\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sxc298pF9m9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_image(img_tensor):\n",
        "  tensor_shape=tf.shape(img_tensor)\n",
        "  element_shape=tf.shape(tensor_shape)\n",
        "  if(element_shape>3):\n",
        "    assert tensor_shape[0]==1\n",
        "    img_tensor=img_tensor[0]\n",
        "  return tf.keras.preprocessing.image.array_to_img(img_tensor)"
      ],
      "metadata": {
        "id": "gDHRlOFe9iul"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  image = tf.io.read_file(path_to_img)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  shape = tf.shape(image)[:-1]\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "uBzNYRdj-1Hb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(content_path, style_path):\n",
        "  content_image = load_img(\"{}\".format(content_path))\n",
        "  style_image = load_img(\"{}\".format(style_path))\n",
        "\n",
        "  return content_image, style_image\n",
        "\n",
        "\n",
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        "def show_images_with_objects(images, titles=[]):\n",
        "  if len(images) != len(titles):\n",
        "    return\n",
        "\n",
        "  plt.figure(figsize=(20, 12))\n",
        "  for idx, (image, title) in enumerate(zip(images, titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    imshow(image, title)\n",
        "\n",
        "\n",
        "def display_gif(gif_path):\n",
        "  with open(gif_path,'rb') as f:\n",
        "    display_fn(Image(data=f.read(), format='png'))\n",
        "\n",
        "\n",
        "def create_gif(gif_path, images):\n",
        "  mimsave(gif_path, images, fps=1)\n",
        "\n",
        "  return gif_path\n",
        "\n",
        "\n",
        "def clip_image_values(image, min_value=0.0, max_value=255.0):\n",
        "  return tf.clip_by_value(image, clip_value_min=min_value, clip_value_max=max_value)\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "sDn3tkH0_Dgt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = 'images'\n",
        "\n",
        "# create directory\n",
        "!mkdir {IMAGE_DIR}\n",
        "\n",
        "# download images to the directory you just created\n",
        "# !wget -q -O ./images/didi.jpg /content/drive/MyDrive/Image/IMG-20240615-WA0019.jpg\n",
        "!wget -q -O ./images/cafe.jpg https://cdn.pixabay.com/photo/2018/07/14/15/27/cafe-3537801_1280.jpg\n",
        "!wget -q -O ./images/dynamite.jpg https://cdn.pixabay.com/photo/2015/10/13/02/59/animals-985500_1280.jpg\n",
        "# !wget -q -O ./images/painting.jpg /content/drive/MyDrive/Image/David_-_Napoleon_crossing_the_Alps_-_Malmaison2.jpg\n",
        "\n",
        "print(\"image files you can choose from: \")\n",
        "!ls images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m76osQBBcUL",
        "outputId": "cda7c96e-f765-4fed-fa8e-53e1f715cb29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image files you can choose from: \n",
            "cafe.jpg  dynamite.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define source and destination paths\n",
        "src_path1 = '/content/drive/MyDrive/Image/Leonardo_da_Vinci_-_Mona_Lisa.jpg'\n",
        "dest_path1 = '/content/images/painting.jpg'\n",
        "src_path2='/content/drive/MyDrive/Image/IMG-20240615-WA0019.jpg'\n",
        "dest_path2='/content/images/didi.jpg'\n",
        "\n",
        "# Copy the file\n",
        "shutil.copy(src_path1, dest_path1)\n",
        "shutil.copy(src_path2, dest_path2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "uEKMteGoGWG7",
        "outputId": "59647990-1848-478d-b42e-bbadf5b45807"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Image/Leonardo_da_Vinci_-_Mona_Lisa.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a7703f8a2cc8>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Copy the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_path1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Image/Leonardo_da_Vinci_-_Mona_Lisa.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = f'{IMAGE_DIR}/cafe.jpg'\n",
        "style_path = f'{IMAGE_DIR}/dynamite.jpg'"
      ],
      "metadata": {
        "id": "9o6BEaKSBuk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the content and style image\n",
        "content_image, style_image = load_images(content_path, style_path)\n",
        "show_images_with_objects([content_image, style_image],\n",
        "                         titles=[f'content image: {content_path}',\n",
        "                                 f'style image: {style_path}'])"
      ],
      "metadata": {
        "id": "AIIbH9ZgBzyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear session to make layer naming consistent when re-running this cell\n",
        "K.clear_session()\n",
        "\n",
        "# download the vgg19 model and inspect the layers\n",
        "tmp_vgg = tf.keras.applications.vgg19.VGG19()\n",
        "tmp_vgg.summary()\n",
        "\n",
        "# delete temporary variable\n",
        "del tmp_vgg"
      ],
      "metadata": {
        "id": "TwR5zic-B09N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# style layers of interest\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "\n",
        "# choose the content layer and put in a list\n",
        "content_layers = ['block5_conv2']\n",
        "\n",
        "# combine the two lists (put the style layers before the content layers)\n",
        "output_layers = style_layers + content_layers\n",
        "\n",
        "# declare auxiliary variables holding the number of style and content layers\n",
        "NUM_CONTENT_LAYERS = len(content_layers)\n",
        "NUM_STYLE_LAYERS = len(style_layers)"
      ],
      "metadata": {
        "id": "rXsEYCPWCtrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg_model(layer_names):\n",
        "\n",
        "  # load the the pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "  # freeze the weights of the model's layers (make them not trainable)\n",
        "  vgg.trainable = False\n",
        "\n",
        "  # create a list of layer objects that are specified by layer_names\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  # create the model that outputs content and style layers only\n",
        "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "kVvj7bTYCvdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "PX1HLPfRNylS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NkSpHDWCxxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear session to make layer naming consistent if re-running the cell\n",
        "K.clear_session()\n",
        "\n",
        "# create a vgg-19 model\n",
        "vgg = vgg_model(output_layers)\n",
        "vgg.summary()"
      ],
      "metadata": {
        "id": "ZkG5UDyZCx3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_loss(features, targets):\n",
        "\n",
        "  # get the average of the squared errors\n",
        "  style_loss = tf.reduce_mean(tf.square(features - targets))\n",
        "\n",
        "  return style_loss"
      ],
      "metadata": {
        "id": "6K0xvHd_CzuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_loss(features, targets):\n",
        "\n",
        "  # get the sum of the squared error multiplied by a scaling factor\n",
        "  content_loss = 0.5 * tf.reduce_sum(tf.square(features - targets))\n",
        "\n",
        "  return content_loss"
      ],
      "metadata": {
        "id": "rV4eIbGMC07P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(input_tensor):\n",
        "\n",
        "  # calculate the gram matrix of the input tensor\n",
        "  gram = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "\n",
        "  # get the height and width of the input tensor\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  height = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  # get the number of locations (height times width), and cast it as a tf.float32\n",
        "  num_locations = tf.cast(height * width, tf.float32)\n",
        "\n",
        "  # scale the gram matrix by dividing by the number of locations\n",
        "  scaled_gram = gram / num_locations\n",
        "\n",
        "  return scaled_gram"
      ],
      "metadata": {
        "id": "pDQowaHoC31I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_layer_list = [layer.output for layer in vgg.layers]\n",
        "tmp_layer_list"
      ],
      "metadata": {
        "id": "YyetqxFUC5fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_image_features(image):\n",
        "\n",
        "  # preprocess the image using the given preprocessing function\n",
        "  preprocessed_style_image = preprocess_image(image)\n",
        "\n",
        "  # get the outputs from the custom vgg model that you created using vgg_model()\n",
        "  outputs = vgg(preprocessed_style_image)\n",
        "\n",
        "  # Get just the style feature layers (exclude the content layer)\n",
        "  style_outputs = outputs[:NUM_STYLE_LAYERS]\n",
        "\n",
        "  # for each style layer, calculate the gram matrix for that layer and store these results in a list\n",
        "  gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs]\n",
        "\n",
        "  return gram_style_features"
      ],
      "metadata": {
        "id": "kU8NVC5LC6sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_image_features(image):\n",
        "\n",
        "  # preprocess the image\n",
        "  preprocessed_content_image = preprocess_image(image)\n",
        "\n",
        "  # get the outputs from the vgg model\n",
        "  outputs = vgg(preprocessed_content_image)\n",
        "\n",
        "  # get the content layers of the outputs\n",
        "  content_outputs = outputs[NUM_STYLE_LAYERS:]\n",
        "\n",
        "  # return the content layer outputs of the content image\n",
        "  return content_outputs"
      ],
      "metadata": {
        "id": "FYZwHxeEC8Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_content_loss(style_targets, style_outputs, content_targets,\n",
        "                           content_outputs, style_weight, content_weight):\n",
        "\n",
        "\n",
        "  # sum of the style losses\n",
        "  style_loss = tf.add_n([ get_style_loss(style_output, style_target)\n",
        "                           for style_output, style_target in zip(style_outputs, style_targets)])\n",
        "\n",
        "  # Sum up the content losses\n",
        "  content_loss = tf.add_n([get_content_loss(content_output, content_target)\n",
        "                           for content_output, content_target in zip(content_outputs, content_targets)])\n",
        "\n",
        "  # scale the style loss by multiplying by the style weight and dividing by the number of style layers\n",
        "  style_loss = style_loss * style_weight / NUM_STYLE_LAYERS\n",
        "\n",
        "  # scale the content loss by multiplying by the content weight and dividing by the number of content layers\n",
        "  content_loss = content_loss * content_weight / NUM_CONTENT_LAYERS\n",
        "\n",
        "  # sum up the style and content losses\n",
        "  total_loss = style_loss + content_loss\n",
        "\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "0Trbt1jaC-OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradients(image, style_targets, content_targets,\n",
        "                        style_weight, content_weight, var_weight):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # get the style image features\n",
        "    style_features = get_style_image_features(image)\n",
        "\n",
        "    # get the content image features\n",
        "    content_features = get_content_image_features(image)\n",
        "\n",
        "    # get the style and content loss\n",
        "    loss = get_style_content_loss(style_targets, style_features, content_targets,\n",
        "                                  content_features, style_weight, content_weight)\n",
        "\n",
        "  # calculate gradients of loss with respect to the image\n",
        "  gradients = tape.gradient(loss, image)\n",
        "\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "Ye36Kyq6C__W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_image_with_style(image, style_targets, content_targets, style_weight,\n",
        "                            var_weight, content_weight, optimizer):\n",
        "  # calculate gradients using the function that you just defined.\n",
        "  gradients = calculate_gradients(image, style_targets, content_targets,\n",
        "                                  style_weight, content_weight, var_weight)\n",
        "\n",
        "  # apply the gradients to the given image\n",
        "  optimizer.apply_gradients([(gradients, image)])\n",
        "\n",
        "  # clip the image using the utility clip_image_values() function\n",
        "  image.assign(clip_image_values(image, min_value=0.0, max_value=255.0))"
      ],
      "metadata": {
        "id": "81OyIBSEDBjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_style_transfer(style_image, content_image, style_weight=1e-2, content_weight=1e-4,\n",
        "                       var_weight=0, optimizer='adam', epochs=1, steps_per_epoch=1):\n",
        "  images = []\n",
        "  step = 0\n",
        "\n",
        "  # get the style image features\n",
        "  style_targets = get_style_image_features(style_image)\n",
        "\n",
        "  # get the content image features\n",
        "  content_targets = get_content_image_features(content_image)\n",
        "\n",
        "  # initialize the generated image for updates\n",
        "  generated_image = tf.cast(content_image, dtype=tf.float32)\n",
        "  generated_image = tf.Variable(generated_image)\n",
        "\n",
        "  # collect the image updates starting from the content image\n",
        "  images.append(content_image)\n",
        "\n",
        "  # incrementally update the content image with the style features\n",
        "  for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "      step += 1\n",
        "\n",
        "      # Update the image with the style using the function that you defined\n",
        "      update_image_with_style(generated_image, style_targets, content_targets,\n",
        "                              style_weight, var_weight, content_weight, optimizer)\n",
        "\n",
        "      print(\".\", end='')\n",
        "\n",
        "      if (m + 1) % 10 == 0:\n",
        "        images.append(generated_image)\n",
        "\n",
        "    # display the current stylized image\n",
        "    clear_output(wait=True)\n",
        "    display_image = tensor_to_image(generated_image)\n",
        "    display_fn(display_image)\n",
        "\n",
        "    # append to the image collection for visualization later\n",
        "    images.append(generated_image)\n",
        "    print(\"Train step: {}\".format(step))\n",
        "\n",
        "  # convert to uint8 (expected dtype for images with pixels in the range [0,255])\n",
        "  generated_image = tf.cast(generated_image, dtype=tf.uint8)\n",
        "\n",
        "  return generated_image, images"
      ],
      "metadata": {
        "id": "8PKFAB0VDDP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define style and content weight\n",
        "style_weight =  2e-2\n",
        "content_weight = 1e-2\n",
        "\n",
        "# define optimizer. learning rate decreases per epoch.\n",
        "adam = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=20.0, decay_steps=100, decay_rate=0.50\n",
        "    )\n",
        ")\n",
        "\n",
        "# start the neural style transfer\n",
        "stylized_image, display_images = fit_style_transfer(style_image=style_image, content_image=content_image,\n",
        "                                                    style_weight=style_weight, content_weight=content_weight,\n",
        "                                                    var_weight=0, optimizer=adam, epochs=20, steps_per_epoch=100)"
      ],
      "metadata": {
        "id": "mSCDx_fVDE5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display GIF of Intermedite Outputs\n",
        "GIF_PATH = 'style_transfer.gif'\n",
        "gif_images = [np.squeeze(image.numpy().astype(np.uint8), axis=0) for image in display_images]\n",
        "gif_path = create_gif(GIF_PATH, gif_images)\n",
        "display_gif(gif_path)"
      ],
      "metadata": {
        "id": "8Ca_IioeDGLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TA6ovh4Hotm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}